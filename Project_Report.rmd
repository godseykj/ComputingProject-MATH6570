---
title: "Project Report"
author: "Jeremiah Allis, Kara Godsey, Mansi Darji, Mian Adnan"
date: "12/11/2021"
output: 
  html_document:
    code_folding: show
    theme: journal
    toc: yes
    toc_float: yes
  word_document:
    toc: yes
---

```{r include=FALSE}
library(nortest)
library(lawstat)
library(gld)
library(VGAM)
library(truncnorm)
library(ggplot2)
library(kableExtra)
library(knitr)
library(dplyr)
```


# Section 1: Introduction
  
Normal distributions are most common distributions used in statistical theory and applications. Performing statistical analysis using parametric methods, validation of the normality assumption must need to be checked. Graphical methods like histogram, Q-Q plot or box plot can be used to check that assumption. Due to subjective nature of these methods, they do not provide the formal conclusive evidence about normality assumption. Formal normality tests come to handy and can be reliable for validating normality assumption. There are a significant number of tests of normality available. But, they are created under certain conditions or assumptions.This paper did the comparison of 8 selected normality tests based on Type I error rate and power of these tests. This report defers to "*Comparisons of various types of normality tests*" for the theory behind each test, while listing what aspect of normality each test assesses below. For every function that was used to run the tests, we compared both the source code and theory of the test to that of the paper's to confirm it was the appropriate test to run. 
  
## Tests

1. **Shapiro-Wilk test (SW test)**
    
    The Shapiro Wilks test assesses the normality by looking at the regression of the data. If the data is normally distributed so that $Y \sim N(\mu,\sigma^2)$, then when a linear regression is applied to the data such that $Y=\mu + \sigma X$, it should hold that $X\sim N(0,1)$. The SW test statistic is designed to be a measure of how far off $X$ is from following the standard normal distribution. The smaller the test statistic, the more likely it is for the data to not be normally distributed.
    
    For this test we used the `shapiro.test` function from the `stats` package that is built in to R.
    
2. **Kolmogorov-Smirnov test (KS test)**
    
    The Kolmogorov Smirnov test is an empirical distribution test where $X_{(1)} < \cdots < X_{(n)}$ is an ordered random sample of size $n$ from $X$ with distribution $F(x)$. This means that it tests the EDF $F_n(x)$, in this case $F_n(x) = \frac{\text{no. of observations} \leq x}{n},  -\infty<x<\infty$. This test requires specific information about some distribution with which the data is being compared. Define this as $F^*(x)$ with specified parameters; in this case we use the normal distribution. The test statistic is defined differently depending on the null hypothesis, but in all methods the supremum of the difference between the defined $F_n(x)$ and $F^*(x)$ is used in testing. The larger the value of the test statistic, the less normal the data is.  
    
    For this test we used the `ks.test` function from the  `stats` package that is built in to R.
    
3. **Lilliefors test (LL test)**

    The Lilliefors test statistic is similar to the KS test statistic except $\mu$ and $\sigma$ are replaced by the sample mean and sample standard deviation for the null distribution $F^*(x)$. The key difference is that the LL test uses an ordered normalized sample of $Z_i$ rather than an ordered random sample of the original $X_i$ values when calculating the test statistic. The method of the test is the same, so large test statistics indicate a non-normal distribution.
    
    For this test we used the `lillie.test` function from the `nortest` package.
    
4. **Cramer-Von mises test (CVM test)**

    The Cramer von Mises test is an alternative test to the KS and LL tests, also based on a random ordered sample. This test judges goodness of fit based on $F^*(x)$ based on the statistic given by $$nw^2=n\int_{-\infty}^{-\infty}[F_n(x)-F*(x)]^2dF(x).$$ This statistic is a measure of the squared difference from the hypothesized distribution. There is an alternative way to calculate this, but this statistic highlights the similarity to the KS test.
    
    For this test we used the `cvm.test` function from the `nortest` package.
    
5. **Anderson-Darling test (AD test)**
    
    The Anderson-Darling test is a modification of the CVM test. The goal of this modification is to provide more weight to the tails of the distribution and use the hypothesized $F^*(x)$ when calculating the critical values. As a result, the test tends to be more sensitive to small changes when compared to the CVM test and the critical values need to be recalculated depending on $F^*(x)$. This deviation from CVM can be seen best in the calculation of the test statistic $$AD =n \int_{-\infty}^{\infty}[F_n(x)-F*(x)]^2 \Psi(F(x))dF(x),$$ where $\Psi(F(x))$ is the weight that differentiates this test from the CVM test. 
    
    For this test we used the `ad.test` function from the `nortest` package.
    
6. **Pearson's Chi-Squared test (CSQ test)**
    
    Pearson's Chi-Squared test is a goodness of fit test based on binning the observations. Thus, it is not recommended for assessing continuous distributions as it counts the observations in each bin and uses that count. Let $O_j$ be the observations in each bin $j$, such that $j=1, 2, ..., c$ and $p^*_j$ be the probability of a random observation being in bin $j$, under the assumption the null hypothesis is true. Then, the expected number of observations in each bin is $E_j=p^*_jn$ for sample size $n$. The CSQ statistic is a measure of how $O_j$ deviates from the expected value. 
    
    There is some maximum number of bins that can be calculated such that $c<M=4(2n^2/z^2_{\alpha})^{\frac{1}{5}}$. In Yap's paper, they ran the CSQ test over every value of possible c and reported the best result. This report applied the CSQ test for a specific value of $c$ such that it followed Scott's Reference Rule. This rule is defined so that $c=2.15\sigma n^{-\frac{1}{5}}$. This change was made to be more realistic with how the working statistician would apply the CSQ test. 
    
    For this test we used the `pearson.test` function from the `nortest` package.
    
7. **Jarque-Bera test (JB test)**

    The Jarque-Bera test is a moments-based test that applies a Langrange multiplier on the Pearson family of distributions to test normality on the regression residuals. The test statistic is based on sample skewness and kurtosis, and follows a Chi-Squared distribution with 2 degrees of freedom. When the distribution of interest has a skewness of 0 and kurtosis of 3, the JB statistic is 0 which implies that the distribution of interest is normally distributed. The larger values of skewness and kurtosis leads to the reject of normality. 
    
    For this test we used the `rjb.test` function from the `lawstat` package.
    
8. **D’Agostino–Pearson Omnibus test (DP test)**

    Similarly to the JB test, the D'Agostino Pearson Omnibus test is also a test of moments. Using the sample skewness and kurtosis, the test statistic calculates the normal approximations of these values and follows a Chi-Squared distribution with 2 degrees of freedom when the population is normally distributed. This test is good as it detects deviation from normality from either skewness or kurtosis. 
    
    For this test, we used a modified version of `dagoTest` from the `fBasics` package. The modifications as well as the code for the function is given in further detail in the Results section below.

### Hypothesis testing for Normality Assumption

**H0**: The null hypothesis will be that the observed, random independent sample of size n or specified distribution comes from a population with a normal distribution. Mathematically, one would write this as $F(x) \sim N(\mu, \sigma^2)$

**H1**: The alternative hypothesis will be that the observed, random independent sample of size n or specified distribution comes from a population with a non-normal distribution. Mathematically, one would write this as $F(x) \not\sim N(\mu, \sigma^2)$

Note: It is important to keep the significance level $\alpha$ consistent between the tests when comparing them. Thus, all the tests will have the same probability of rejecting the null hypothesis when the distribution is normal, this means that each test will have the same Type I error. 

## Distributions

1. **Uniform**: R function `runif` is used to generate the random variates from uniform distribution.  

2. **Generalized Lambda Distribution**: R function `rgl` is used to generate the random variates for this  distribution for given paramater values. Additionally, the function `gld.moments` is used to compute the theoretical mean and variance. Each of these functions are from the `gld` package.

3. **Truncated Normal**: R function `rtruncnorm` is used to generate the random deviates for this distribution.

4. **Scale Contaminated Normal: ScConN(p,b):** We have created the function below to generate the random variates from this distribution.

```{r}
rScConN <- function(n,p,b){
  k <- runif(n)
  k <- ifelse(k <= p, 1, 0)
  ScConNdist <- k*rnorm(n, mean = 0, sd=b) + (1-k)*rnorm(n)
  ScConNdist
}

```


5. **t distribution**: R function `rt` is used to generate the random deviates from this distribution. 

6. **Logistic**: R function `rlogis` is used to generate random deviates from this distribution.
 
7. **Laplace**: R function `rlaplace` is used to generate random deviates from this distribution.

8. **Chi Squared**: R function `rchisq` is used to generate random deviates from this distribution.

9. **Beta**: R function `rbeta` is used to generate random deviates from this distribution.

10. **Weibull**: R function `rweibull` is used to generate random deviates from this distribution.

11. **Lognormal**: We have created the function below to generate the random variates from this distribution.

```{r}
rlog <- function(n){
  x <- rnorm(n,0,1)
  dist <- exp(x)
}
```

12. **Location Contaminated Normal: LocConN(p,a)**: We have created the function below to generate the random variates from this distribution.

```{r}
rLoConN <- function(n,p,a){
  k <- runif(n)
  k <- ifelse(k <= p, 1, 0)
  LoConNdist <- k*rnorm(n, mean = a, sd=1) + (1-k)*rnorm(n)
  LoConNdist
}

```

---

# Section 2: Results

## Process

In order to replicate the paper, we employed the following process. First, we computed critical values for each test. Next, we performed Monte Carlo simulations with each alternative distribution. For each distribution, 10,000 simulations were conducted and test statistics were computed in each iteration. Comparing each test statistic to the critical value, we then computed the power for each test with the given alternative distribution. Finally, we display these results in graph or table form to compare with the results found in the paper. For our power comparison, we checked each of our estimates to see if it was within two standard deviations of the estimate given in the paper.

In terms of test statistics, there were a few specifics that are of note. For the CSQ test, we used a different approach than the paper to compute the number of bins. Rather than looping through the possible bin sizes from 1 to $c$, we used the function `nclass.scott` to compute the number of bins using Scott's reference rule. For the DP omnibus test, we have modified the source code found in `fBasics` so that the function computes the statistic for sample sizes less than 20. For the JB test, we used the non-robust version of the test statistic, because its results followed the paper more closely than the robust version of the test. The modified DP omnibus test is shown below.

```{r eval=FALSE}
# Edited from dagoTest(x) in fBasics library. 
omnibus.test <- function(x){
  # Internal Function for D'Agostino Normality Test:
  
  # FUNCTION:
  if (exists("complete.cases")) {
    test = complete.cases(x)
  } else {
    test = !is.na(x)
  }
  x = x[test]
  n = length(x)
  meanX = mean(x)
  s =  sqrt(mean((x-meanX)**2))
  a3 = mean((x-meanX)**3)/s**3
  a4 = mean((x-meanX)**4)/s**4
  SD3 = sqrt(6*(n-2)/((n+1)*(n+3)))
  SD4 = sqrt(24*(n-2)*(n-3)*n/((n+1)**2*(n+3)*(n+5)))
  U3 = a3/SD3
  U4 = (a4-3+6/(n+1))/SD4
  b  = (3*(n**2+27*n-70)*(n+1)*(n+3))/((n-2)*(n+5)*(n+7)*(n+9))
  W2 = sqrt(2*(b-1))-1
  delta = 1/sqrt(log(sqrt(W2)))
  a = sqrt(2/(W2-1))
  Z3 = delta*log((U3/a)+sqrt((U3/a)**2+1))
  B = (6*(n*n-5*n+2)/((n+7)*(n+9)))*sqrt((6*(n+3)*(n+5))/(n*(n-2)*(n-3)))
  A = 6+(8/B)*((2/B)+sqrt(1+4/(B**2)))
  jm = sqrt(2/(9*A))
  pos = ((1-2/A)/(1+U4*sqrt(2/(A-4))))**(1/3)
  Z4 = (1-2/(9*A)-pos)/jm
  omni = Z3**2+Z4**2
  
  # Result:
  omni
}
```

### Obtaining the critical values

To obtain the critical values, we generated 50,000 samples from a normal distribution using the `rnorm` function and computed the test statistic for each test. The 50,000 test statistics were then ordered to create an empirical distribution. The critical value then corresponds to the $\alpha$ quantile (left-tailed test such as SW) or $1-\alpha$ quantile (right-tailed test such as LL) of the empirical distribution. This process was repeated for each sample size. The result is a single critical value for each test at each sample size; these critical values are shown in the table below.

```{r eval=FALSE}
alpha <- 0.05
ssizes <- c(10, 15, 20, 25, 30, 40, 50, 100, 200, 300, 400, 500, 1000, 1500, 2000)

# Obtaining Critical Values - Found in generalpowercomputations.r

SW <- numeric()
KS <- numeric()
LL <- numeric()
AD <- numeric()
DP <- numeric()
JB <- numeric()
CVM <- numeric()
CSQ <- numeric()
SWcrit <- numeric()
KScrit  <- numeric()
LLcrit  <- numeric()
ADcrit <- numeric()
DPcrit <- numeric()
JBcrit <- numeric()
CVMcrit <- numeric()
CSQcrit <- numeric()
for (a in 1:15){
  
  for (i in 1:50000){
    x <- rnorm(ssizes[a], 0, 1)
    SW[i] <- shapiro.test(x)$statistic
    KS[i] <- ks.test(x, "pnorm")$statistic
    LL[i] <- lillie.test(x)$statistic 
    AD[i] <- ad.test(x)$statistic
    DP[i]<- omnibus.test(x)$statistic
    JB[i] <- rjb.test(x,option="JB")$statistic
    CVM[i] <- cvm.test(x)$statistic
    
    c <- nclass.scott(x)
    CSQ[i] <- pearson.test(x, n.classes=c)$statistic
  }
  
  SW <- sort(SW)
  KS <- sort(KS)
  LL <- sort(LL)
  AD <- sort(AD)
  DP <- sort(DP)
  JB <- sort(JB)
  CVM <- sort(CVM)
  CSQ <- sort(CSQ)
  
  SWcrit[a] <- quantile(SW, alpha)
  KScrit[a]  <- quantile(KS, 1-alpha)
  LLcrit[a]  <- quantile(LL, 1-alpha)
  ADcrit[a] <- quantile(AD, 1-alpha)
  DPcrit[a] <- quantile(DP, 1-alpha)
  JBcrit[a] <- quantile(JB, 1-alpha)
  CVMcrit[a] <- quantile(CVM, 1-alpha)
  CSQcrit[a] <- quantile(CSQ, 1-alpha)
}
Critvaluematrix <- cbind(ssizes, SWcrit, KScrit, LLcrit, ADcrit, DPcrit, JBcrit, CVMcrit, CSQcrit)
write.table(Critvaluematrix, "critical_values.csv")
```

```{r}
x <- read.table("critical_values.csv")
kable(x,"pipe", caption="Critical Value Matrix")
```


### Evaluating the Power of each distribution

To calculate the power of each test given the distribution, we wrote a function called `getpower`. The function takes as inputs the theoretical mean and variance of the distribution, the r function used to generate a sample from the distribution, and any additional parameters that the generation function requires, such as $\alpha$ and $\beta$ for the weibull distribution. The theoretical mean and variance are required for the KS test, since we standardize the data prior to passing it to the KS test function. 

We begin by generating 10,000 simulated samples from the given distribution, using the functions that are discussed in the Introduction section above. Next, for each sample we calculated the eight test statistics. Then, we compared the test statistic value with the critical value for the given test. For the SW test, we evaluated whether the statistic was less than or equal to the critical value. For all other tests, we evaluated whether the statistic was greater than or equal to the critical value (with the exception of the KS test for which we used the two-tail method). We then assigned a 1 or 0 based on the result of the test. The power of the normality test was then computed by summing the vector of 1s and 0s and dividing by the number of samples (10,000). In this way, the power of the normality test for a particular sample size is the proportion of samples for which the test rejected the null hypothesis of normality. This process was repeated for each sample size. The function is shown below.

```{r eval=FALSE}
# Power Function - Found in powerfunction.r

cvalues <- read.table("critical_values.csv",header=TRUE)
ssizes <- cvalues[,"ssizes"]; SWcrit <- cvalues[,"SWcrit"]; LLcrit <- cvalues[,"LLcrit"]; 
KScrit <- cvalues[,"KScrit"]; ADcrit <- cvalues[,"ADcrit"]; JBcrit <- cvalues[,"JBcrit"]; 
CVMcrit <- cvalues[,"CVMcrit"]; DPcrit <- cvalues[,"DPcrit"]; CSQcrit <- cvalues[,"CSQcrit"]
getpower <- function(distmean, distvar, dist_function, p1=NULL, p2=NULL, p3=NULL, p4=NULL){
  powerSW <- c()
  powerKS <- c()
  powerLL <- c()
  powerAD <- c()
  powerDP <- c()
  powerJB <- c()
  powerCVM <- c()
  powerCSQ <- c()
  distsd <- sqrt(distvar)
  
  for (a in 1:15){
    testSW <- numeric()
    testKS <- numeric()
    testLL <- numeric()
    testAD <- numeric()
    testDP <- numeric()
    testJB <- numeric()
    testCVM <- numeric()
    testCSQ <- numeric()
    for(i in 1:10000) { 
      n <- ssizes[a]
      if(is.null(p1)==FALSE){
        if(is.null(p2)==FALSE){
          if(is.null(p3)==FALSE){
            if(is.null(p4)==FALSE){
              dist <- as.vector(sapply(n, noquote(dist_function), p1, p2, p3, p4))
            } else{dist <- as.vector(sapply(n, noquote(dist_function), p1, p2, p3))}
          } else{dist <- as.vector(sapply(n, noquote(dist_function), p1, p2))}
        } else{dist <- as.vector(sapply(n, noquote(dist_function), p1))}
      } else{dist <- as.vector(sapply(n, noquote(dist_function)))}
      distKS <- (dist-distmean)/distsd
      SWd <- shapiro.test(dist)$statistic 
      KSd <- ks.test(distKS, "pnorm")$statistic
      LLd <- lillie.test(dist)$statistic 
      ADd <- ad.test(dist)$statistic
      DPd <- omnibus.test(dist)
      JBd <- rjb.test(dist, "JB")$statistic
      CVMd <- cvm.test(dist)$statistic
      c <- nclass.scott(dist)
      CSQd <- pearson.test(dist, n.classes=c)$statistic
      
      ifelse(SWd <= SWcrit[a], testSW[i] <- 1, testSW[i] <- 0)
      ifelse(KSd >= KScrit[a] | KSd <= -KScrit[a], testKS[i] <- 1, testKS[i] <- 0)
      ifelse(LLd >= LLcrit[a], testLL[i] <- 1, testLL[i] <- 0)
      ifelse(ADd >= ADcrit[a], testAD[i] <- 1, testAD[i] <- 0)
      ifelse(DPd >= DPcrit[a], testDP[i] <- 1, testDP[i] <- 0)
      ifelse(JBd >= JBcrit[a], testJB[i] <- 1, testJB[i] <- 0)
      ifelse(CVMd >= CVMcrit[a], testCVM[i] <- 1, testCVM[i] <- 0)
      ifelse(CSQd >= CSQcrit[a], testCSQ[i] <- 1, testCSQ[i] <- 0)
      
    }
    powerSW <- c(powerSW, sum(testSW)/10000)
    powerKS <- c(powerKS, sum(testKS)/10000)
    powerLL <- c(powerLL, sum(testLL)/10000)
    powerAD <- c(powerAD, sum(testAD)/10000)
    powerDP <- c(powerDP, sum(testDP)/10000)
    powerJB <- c(powerJB, sum(testJB)/10000)
    powerCVM <- c(powerCVM, sum(testCVM)/10000)
    powerCSQ <- c(powerCSQ, sum(testCSQ)/10000)
  }
  powermatrix <<- cbind(ssizes, powerSW, powerKS, powerLL, powerAD, powerDP, powerJB, powerCVM, powerCSQ)
}
```

This is an example of the `getpower` function used to find the power of the GLD (0, 1, 0.75, 0.75) distribution. The `gld.moments` function from the `GLD` package is used to compute the theoretical mean and variance, the first two arguments of the `getpower` function. 

```{r eval=FALSE}
lam1 <- 0; lam2 <- 1; lam3 <- 0.75; lam4 <- 0.75
gldmean <- gld.moments(c(lam1,lam2,lam3,lam4))[1]
gldvar <- gld.moments(c(lam1,lam2,lam3,lam4))[2]
GLD1a <- getpower(gldmean, gldvar, "rgl",lam1, lam2, lam3, lam4)
```

The output of the `getpower` function is a matrix of the power for each test and sample size. After running the function, we saved the matrix to a csv file. Below is a table of the power for the GLD (0, 1, 0.75, 0.75) distribution that was used above as a demonstration.

```{r}
y <- read.table("Figure1/gld-a.csv")
kable(y, "pipe", caption="GLD(0,1,0.75,0.75) power")
```

### Power Function in Action

Listed below is the code that was run for each distribution to calculate power.

```{r eval=FALSE}
# Power calculation for every distribution

# Table 2 Short Tailed Distributions

# Uniform 
Uniformpower <- getpower(0, 1, "runif")

# Tukey(0, 1, 1.25, 1.25)
lam1 <- 0; lam2 <- 1; lam3 <- 1.25; lam4 <- 1.25
gldmean <- gld.moments(c(lam1,lam2,lam3,lam4))[1]
gldvar <- gld.moments(c(lam1,lam2,lam3,lam4))[2]
system.time(Tukeypower <- getpower(gldmean, gldvar, "rgl",lam1, lam2, lam3, lam4))

# Trunc(-2, 2)
Truncpower <- getpower(0, 1, "rtruncnorm", -2, 2)

# Table 3 Long Tailed Distributions

#t distribution - t(15)
tmean <- 0
df <- 15
tvar <- df / (df-2)
Tpower <- getpower(tmean, tvar, "rt", df)

#logistic  (Standard)
logmean <- 0
logvar <- (pi^2)/3
logsd <- sqrt(logvar)
Logisticpower <- getpower(logmean, logvar, "rlogis")

#Laplace (Standard)
laplacemean <- 0
laplacevar <- 2
laplacesd <- sqrt(laplacevar)
Laplacepower <- getpower(laplacemean, laplacevar, "rlaplace")

# Asymptotic Distributions Table 4

#weibull(3,1)
alpha <- 3; beta <- 1
wmean <- beta*gamma(1 + (1/alpha))
wvar <- (beta^2)*(gamma(1+(2/alpha))-(gamma(1 + (1/alpha)))^2)
weibullpower <- getpower(wmean, wvar, "rweibull",alpha,beta)

#lognormal (standard)
logpower <- getpower(exp(1/2),exp(2)-exp(1),"rlog")

#LoConN(0.2,3)
p <- 0.2
a <- 3
meanN1 <- a; meanN2 <- 0; varN1 <- 1; varN2 <- 1
meanlcn <- p*meanN1 + (1-p)*meanN2
varlcn <- (p^2)*varN1 + ((1-p)^2)*varN2
LCnom <- getpower(meanlcn, varlcn, "rLoConN", p, a)

# Figure 1

#a - GLD(0,1,0.75,0.75)
lam1 <- 0; lam2 <- 1; lam3 <- 0.75; lam4 <- 0.75
gldmean <- gld.moments(c(lam1,lam2,lam3,lam4))[1]
gldvar <- gld.moments(c(lam1,lam2,lam3,lam4))[2]
GLD1a <- getpower(gldmean, gldvar, "rgl",lam1, lam2, lam3, lam4)

#b - GLD(0,1,0.5,0.5)
lam1 <- 0; lam2 <- 1; lam3 <- 0.5; lam4 <- 0.5
gldmean <- gld.moments(c(lam1,lam2,lam3,lam4))[1]
gldvar <- gld.moments(c(lam1,lam2,lam3,lam4))[2]
GLD1b <- getpower(gldmean, gldvar, "rgl",lam1, lam2, lam3, lam4)

#c - GLD(0,1,0.25,0.25)
lam1 <- 0; lam2 <- 1; lam3 <- 0.25; lam4 <- 0.25
gldmean <- gld.moments(c(lam1,lam2,lam3,lam4))[1]
gldvar <- gld.moments(c(lam1,lam2,lam3,lam4))[2]
GLD1c <- getpower(gldmean, gldvar, "rgl", lam1, lam2, lam3, lam4)

# Figure 2 

#a - GLD(0,1,-0.1,-0.1)
lam1 <- 0; lam2 <- 1; lam3 <- -0.10; lam4 <- -0.10
gldmean <- gld.moments(c(lam1,lam2,lam3,lam4))[1]
gldvar <- gld.moments(c(lam1,lam2,lam3,lam4))[2]
GLD2a <- getpower(gldmean, gldvar, "rgl", lam1, lam2, lam3, lam4)

# Scale Contanimated Normal (0.05, 3)
p <- 0.05
b <- 3
meanN1 <- 0; meanN2 <- 0; varN1 <- b; varN2 <- 1
meanscn <- p*meanN1 + (1-p)*meanN2
varscn <- (p^2)*varN1 + ((1-p)^2)*varN2
SCnom <- getpower(meanscn, varscn, "rScConN", p, b)

#c - GLD(0,1,-0.15,-0.15)
lam1 <- 0; lam2 <- 1; lam3 <- -0.15; lam4 <- -0.15
gldmean <- gld.moments(c(lam1,lam2,lam3,lam4))[1]
gldvar <- gld.moments(c(lam1,lam2,lam3,lam4))[2]
GLD2c <- getpower(gldmean, gldvar, "rgl", lam1, lam2, lam3, lam4)

# Figure 3

# Chi(4df)
df <- 4
chimean <- df
chivar <- 2*df
chipower <- getpower(chimean, chivar, "rchisq",df)

# Beta(2,1)
alpha <- 2
beta <- 1
betamean <- alpha / (alpha + beta)
betavar <- (alpha*beta) / (((alpha+beta)^2)*(alpha+beta+1))
betapower <- getpower(betamean, betavar, "rbeta",alpha, beta)
```

### Function to Graph

Next, we wrote a function to graph the power curve using `ggplot`. The function takes as input the filename of the csv file where the power matrix is saved as well as the name of the distribution as a character which will serve as the title of the graph. The color coding of the tests should align closely with the color coding found in the paper.

```{r eval=FALSE}
# Graphing the power curves for the necessary distributions - Found in powertable.r

graph <- function(filename, distribution){
  table <- read.table(filename)
  data <- data.frame(table)
  ss <- data[,1]
  
  colors <- c("SW"="steelblue","KS"="red3","LL"="green3","AD"="blueviolet",
              "JB"="orange","CVM"="mediumpurple1", "DP"="cyan3", "CSQ"="rosybrown2")
  
  ggplot(data, aes(x=seq(1,15), y=seq(0,1,10))) + 
    geom_line(aes(y=powerSW, color="SW"), size=2) +
    geom_point(aes(y=powerSW, color="SW"), size=3.5) +
    geom_line(aes(y=powerKS, color="KS"),size=2) + 
    geom_point(aes(y=powerKS, color="KS"), size=3.5) +
    geom_line(aes(y=powerLL, color="LL"),size=2) +
    geom_point(aes(y=powerLL, color="LL"), size=3.5) +
    geom_line(aes(y=powerAD, color="AD"),size=2) +
    geom_point(aes(y=powerAD, color="AD"), size=3.5) +
    geom_line(aes(y=powerJB, color="JB"),size=2) +
    geom_point(aes(y=powerJB, color="JB"), size=3.5) +
    geom_line(aes(y=powerCVM, color="CVM"),size=2) +
    geom_point(aes(y=powerCVM, color="CVM"), size=3.5) +
    geom_line(aes(y=powerDP, color="DP"),size=2) +
    geom_point(aes(y=powerDP, color="DP"), size=3.5) +
    geom_line(aes(y=powerCSQ, color="CSQ"),size=2) +
    geom_point(aes(y=powerCSQ, color="CSQ"), size=3.5) +
    labs(x="sample size", y="power",title = distribution, color="Legend") +
    scale_x_continuous(breaks=seq(1,15),labels=c("10","15","20","25","30","40","50","100","200","300","400","500","1000","1500","2000")) +
    theme(plot.title = element_text(hjust = 0.5)) +
    scale_color_manual(values=colors)
}
```

The code below was used to replicate all the graphs found in the paper.

```{r eval=FALSE}
graph("Figure1/gld-a.csv", "GLD(0,1,0.75,0.75)")
graph("Figure1/gld-b.csv", "GLD(0,1,0.5,0.5)")
graph("Figure1/gld-c.csv", "GLD(0,1,0.25,0.25)")
graph("Figure2/gld-a-figure2.csv", "GLD(0,1,-0.1,-0.1)")
graph("Figure2/scnom-b-figure2.csv", "ScConN(0.05,3)")
graph("Figure2/gld-c-figure2.csv", "GLD(0,1,−0.15,−0.15)")
graph("Figure3/chi.csv", "CHI(4df)")
graph("Figure3/beta.csv", "BETA(2,1)")
```

This is an example of the graph produced for the GLD (0, 1, 0.75, 0.75) distribution.

![](Figure1/fig1plotA.jpeg)

### Approval Matrix / Error Analysis

For Error Analysis, first remember how the power of each function was calculated. Let $Y$ be a random variable such that $Y=1$ when a test statistic is rejected and $Y=0$ when the test statistic is not rejected. Then, we have a Bernoulli trial-  but this experiment ran 10000 times, so really it is a Binomial distribution. Thus, there is some "success" probability $\hat{p}$ with our trials. This probability is conveniently already calculated- it is the power of the tests. Thus, an interval that was 3 standard errors large was made for each value in every table in Yap's paper, and the experimental data was checked to see if it fell within that interval. The choice of 3 standard errors was to ensure that every reasonable value was caught, anything further away and the argument that the results do not match is very strong. For the last step, a new Approval Matrix was calculated, denoting a 1 where the power successfully fell within their data, and a 0 otherwise.


In the code below, a function was not written to do this, as there were few distributions with tables. The data was just copied and pasted into R and then replaced to be a matrix titled "Theory". If we had their values for every distribution in tables, it probably would have saved a lot of time to make a function, but it did not seem necessary in this case. After the Theory matrix has been assigned, you need to take the sample sizes used in that table as the reported sample sizes were not consistent through each table. Let $P_{ij}$ be the power in the $i^{th}$ row and $j^{th}$ column in each table, then lower bound and upper bound matrices were calculated in the form of $(P_{ij}-3 \sqrt{\frac{P_{ij}(1-P_{ij})}{10000}}, P_{ij}+3 \sqrt{\frac{P_{ij}(1-P_{ij})}{10000}})$, which can be denoted $(L_{ij}, U_{ij})$. Next, the experimental data was read into R cut down by the sample sizes that were in the paper's table. Then, an if statement was used to compare the experimental data, if it was found that $(L_{ij}<\hat{P}_{ij}< U_{ij}$, then a 1 was put $i^{th}$ row and $j^{th}$ column in the approval matrix and a 0 otherwise. 

```{r eval=FALSE}
# Tables were made based on outputs from power function using knitr::kable. - Found in Project_Report.rmd

# Error Analysis of Tabled Data.  - Example of 1 Distribution. - Found in ErrorAnalysis.r

# This data was from Yap's paper, we just switched values around for each distribution.
Theory <- matrix(
  c(10, 0.1381, 0.0714, 0.1188, 0.1365, 0.1111, 0.1157, 0.1329, 0.0000,
    20, 0.2558, 0.0965, 0.217, 0.2634, 0.1448, 0.1615, 0.2559, 0.0279,
    30, 0.3874, 0.1275, 0.3183, 0.4082, 0.181, 0.2093, 0.3886, 0.0832,
    50, 0.6149, 0.189, 0.5134, 0.6502, 0.2957, 0.3651, 0.6248, 0.1966,
    100, 0.9285, 0.3382, 0.8463, 0.9425, 0.681, 0.7476, 0.9294, 0.4663,
    300, 0.9999, 0.8235, 0.9995, 1.0000, 0.9994, 0.9997, 1.0000, 0.9129,
    500, 1.0000, 0.9789, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9885,
    1000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
    2000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000),
  nrow=9,
  ncol=9,
  byrow=TRUE)
ss <- Theory[, 1]

# Upper/Lower bound for the intervals based on theoretical means from the paper. 
upperbound <- matrix(NA, 9, 9)
upperbound[,1] <- ss
for (i in 1:9){
  for (j in 2:9){
    upperbound[i, j] <- Theory[i, j]+3*sqrt(Theory[i, j]*(1-Theory[i, j])/10000)
  }
}
lowerbound <- matrix(NA, 9, 9)
lowerbound[,1] <- ss
for (i in 1:9){
  for (j in 2:9){
    lowerbound[i, j] <- Theory[i, j]-3*sqrt(Theory[i, j]*(1-Theory[i, j])/10000)
  }
}

# Grabbing our experimental results
Experimental <- read.table("Table4/loconnorm.csv")
Experimental <- Experimental[Experimental$ssizes %in% ss,]

# Comparison of experimental and theoretical values.
Approval <- matrix(NA, 9, 9)
Approval[, 1] <- ss
for (i in 1:9){
  for (j in 2:9){
    b <- upperbound[i, j]
    c <- lowerbound[i, j]
    d <- Experimental[i, j]
    ifelse(d<=b & d>=c, Approval[i, j] <- 1, Approval[i, j] <- 0)
  }
}

colnames(Approval) <- c("Sample", "SW", "KS", "LL", "AD", "DP", "JB", "CVM", "CSQ")
write.table(Approval, "Approval/LocConNorm.csv")


```

This is an example approval matrix, ,for the Location Contaminated Normal distribution. From looking at the approval matrix, it is clear that for this distribution the KS tests and CSQ tests did not replicate the results of the paper and while the other tests missed some values, they did well otherwise. Using this method, it made it very clear to see where the experimental results matched the results in the paper and where they did not. 

```{r}
x <- read.table("Approval/LocConNorm")
kable(x, "pipe", caption="Approval Matrix for Location Contaminated Normal")
```


## Tables

Given below are several tables. The tables on the left are those from the paper. The tables on the right are the power estimates that we produced. In each table, a random estimate was highlighted in red to allow for a quick comparison. There were three tables of power estimates given in the paper: Table 2, Table 3, and Table 4. The tables below are grouped accordingly.

### Table 2

```{r}
titles <- c("n", "SW", "KS", "LL", "AD", "DP", "JB", "CVM", "CSQ")
x <- read.table("TablesFromPaper/2_1.csv")
colnames(x) <- titles
sizes <- x$n
y <- read.table("Table2/t2-uniform.csv")
colnames(y) <- titles
y <- y[y$n %in% sizes,]
```


```{r echo=FALSE}
x[5,3] <- cell_spec(x[5,3], format="html", color="red")
x %>%
  kable("html", align = 'c', caption = 'Uniform(0,1) from Paper', row.names=FALSE, escape=F) %>%
    kable_styling(full_width = F, position = "float_left", font_size=10) #%>% 
  #row_spec(5, bold=T, color="black", background="lightblue")

y[5,3] <- cell_spec(y[5,3], format="html", color = "red") 
y %>%
  kable("html", align = 'c', caption = 'Uniform(0,1) from Our Project', row.names=FALSE, escape=F) %>%
    kable_styling(full_width = F, position = "right", font_size=10) #%>% 
  #row_spec(5, bold = T, color = "black", background = "lightblue")
```

```{r echo=FALSE}
x <- read.table("TablesFromPaper/2_2.csv")
colnames(x) <- titles
sizes <- x$n
y <- read.table("Table2/t2-genlam.csv")
colnames(y) <- titles
y <- y[y$n %in% sizes,]
```

```{r echo=FALSE}
x[4,4] <- cell_spec(x[4,4], format="html", color="red")
x %>%
  kable("html", align = 'c', caption = 'GLD(0, 1, 1.25, 1.25) from Paper', row.names=FALSE, escape=F) %>%
    kable_styling(full_width = F, position = "float_left", font_size=10) #%>% 
  #row_spec(5, bold=T, color="black", background="lightblue")

y[4,4] <- cell_spec(y[4,4], format="html", color = "red") 
y %>%
  kable("html", align = 'c', caption = 'GLD(0, 1, 1.25, 1.25) from Our Project', row.names=FALSE, escape=F) %>%
    kable_styling(full_width = F, position = "right", font_size=10) #%>% 
  #row_spec(5, bold = T, color = "black", background = "lightblue")
```

```{r echo=FALSE}
x <- read.table("TablesFromPaper/2_3.csv")
colnames(x) <- titles
sizes <- x$n
y <- read.table("Table2/t2-truncnorm.csv")
colnames(y) <- titles
y <- y[y$n %in% sizes,]
```

```{r echo=FALSE}
x[6,6] <- cell_spec(x[6,6], format="html", color="red")
x %>%
  kable("html", align = 'c', caption = 'TRUNC(-2,2) from Paper', row.names=FALSE, escape=F) %>%
    kable_styling(full_width = F, position = "float_left", font_size=10) #%>% 
  #row_spec(5, bold=T, color="black", background="lightblue")

y[6,6] <- cell_spec(y[6,6], format="html", color = "red") 
y %>%
  kable("html", align = 'c', caption = 'TRUNC(-2,2) from Our Project', row.names=FALSE, escape=F) %>%
    kable_styling(full_width = F, position = "right", font_size=10) #%>% 
  #row_spec(5, bold = T, color = "black", background = "lightblue")
```

### Table 3

```{r echo=FALSE}
x <- read.table("TablesFromPaper/3_1.csv")
colnames(x) <- titles
sizes <- x$n
y <- read.table("Table3/t3-t15.csv")
colnames(y) <- titles
y <- y[y$n %in% sizes,]
```

```{r echo=FALSE}
x[8,3] <- cell_spec(x[8,3], format="html", color="red")
x %>%
  kable("html", align = 'c', caption = 't(15) from Paper', row.names=FALSE, escape=F) %>%
    kable_styling(full_width = F, position = "float_left", font_size=10) #%>% 
  #row_spec(5, bold=T, color="black", background="lightblue")

y[8,3] <- cell_spec(y[8,3], format="html", color = "red") 
y %>%
  kable("html", align = 'c', caption = 't(15) from Our Project', row.names=FALSE, escape=F) %>%
    kable_styling(full_width = F, position = "right", font_size=10) #%>% 
  #row_spec(5, bold = T, color = "black", background = "lightblue")
```

```{r echo=FALSE}
x <- read.table("TablesFromPaper/3_2.csv")
colnames(x) <- titles
sizes <- x$n
y <- read.table("Table3/t3-logistic.csv")
colnames(y) <- titles
y <- y[y$n %in% sizes,]
```

```{r echo=FALSE}
x[4,8] <- cell_spec(x[4,8], format="html", color="red")
x %>%
  kable("html", align = 'c', caption = 'Logistic from Paper', row.names=FALSE, escape=F) %>%
    kable_styling(full_width = F, position = "float_left", font_size=10) #%>% 
  #row_spec(5, bold=T, color="black", background="lightblue")

y[4,8] <- cell_spec(y[4,8], format="html", color = "red") 
y %>%
  kable("html", align = 'c', caption = 'Logistic from Our Project', row.names=FALSE, escape=F) %>%
    kable_styling(full_width = F, position = "right", font_size=10) #%>% 
  #row_spec(5, bold = T, color = "black", background = "lightblue")
```

```{r echo=FALSE}
x <- read.table("TablesFromPaper/3_3.csv")
colnames(x) <- titles
sizes <- x$n
y <- read.table("Table3/t3-laplace.csv")
colnames(y) <- titles
y <- y[y$n %in% sizes,]
```

```{r echo=FALSE}
x[6,6] <- cell_spec(x[6,6], format="html", color="red")
x %>%
  kable("html", align = 'c', caption = 'Laplace from Paper', row.names=FALSE, escape=F) %>%
    kable_styling(full_width = F, position = "float_left", font_size=10) #%>% 
  #row_spec(5, bold=T, color="black", background="lightblue")

y[6,6] <- cell_spec(y[6,6], format="html", color = "red") 
y %>%
  kable("html", align = 'c', caption = 'Laplace from Our Project', row.names=FALSE, escape=F) %>%
    kable_styling(full_width = F, position = "right", font_size=10) #%>% 
  #row_spec(5, bold = T, color = "black", background = "lightblue")
```

### Table 4

```{r echo=FALSE}
x <- read.table("TablesFromPaper/4_1.csv")
colnames(x) <- titles
sizes <- x$n
y <- read.table("Table4/weibull.csv")
colnames(y) <- titles
y <- y[y$n %in% sizes,]
```

```{r echo=FALSE}
x[1,3] <- cell_spec(x[1,3], format="html", color="red")
x %>%
  kable("html", align = 'c', caption = 'Weibull from Paper', row.names=FALSE, escape=F) %>%
    kable_styling(full_width = F, position = "float_left", font_size=10) #%>% 
  #row_spec(5, bold=T, color="black", background="lightblue")

y[1,3] <- cell_spec(y[1,3], format="html", color = "red") 
y %>%
  kable("html", align = 'c', caption = 'Weibull from Our Project', row.names=FALSE, escape=F) %>%
    kable_styling(full_width = F, position = "right", font_size=10) #%>% 
  #row_spec(5, bold = T, color = "black", background = "lightblue")
```

```{r echo=FALSE}
x <- read.table("TablesFromPaper/4_2.csv")
colnames(x) <- titles
sizes <- x$n
y <- read.table("Table4/t4-lognormal.csv")
colnames(y) <- titles
y <- y[y$n %in% sizes,]
```

```{r echo=FALSE}
x[6,2] <- cell_spec(x[6,2], format="html", color="red")
x %>%
  kable("html", align = 'c', caption = 'Lognormal from Paper', row.names=FALSE, escape=F) %>%
    kable_styling(full_width = F, position = "float_left", font_size=10) #%>% 
  #row_spec(5, bold=T, color="black", background="lightblue")

y[6,2] <- cell_spec(y[6,2], format="html", color = "red") 
y %>%
  kable("html", align = 'c', caption = 'Lognormal from Our Project', row.names=FALSE, escape=F) %>%
    kable_styling(full_width = F, position = "right", font_size=10) #%>% 
  #row_spec(5, bold = T, color = "black", background = "lightblue")
```

```{r echo=FALSE}
x <- read.table("TablesFromPaper/4_3.csv")
colnames(x) <- titles
sizes <- x$n
y <- read.table("Table4/loconnorm.csv")
colnames(y) <- titles
y <- y[y$n %in% sizes,]
```

```{r echo=FALSE}
x[3,4] <- cell_spec(x[3,4], format="html", color="red")
x %>%
  kable("html", align = 'c', caption = 'LoConN(0.2, 3) from Paper', row.names=FALSE, escape=F) %>%
    kable_styling(full_width = F, position = "float_left", font_size=10) #%>% 
  #row_spec(5, bold=T, color="black", background="lightblue")

y[3,4] <- cell_spec(y[3,4], format="html", color = "red") 
y %>%
  kable("html", align = 'c', caption = 'LoConN(0.2, 3) from Our Project', row.names=FALSE, escape=F) %>%
    kable_styling(full_width = F, position = "right", font_size=10) #%>% 
  #row_spec(5, bold = T, color = "black", background = "lightblue")
```

## Graphs
  
The following graphs were produced for visual comparison with the findings of the paper. The graphs from the paper are given on the left, while our power curves are shown on the right. There were three figures of power curves given in the paper: Figure 1, Figure 2, and Figure 3. The graphs below are grouped accordingly. It was difficult to match the scale of the original graphs in the paper when using ggplot, but the way we matched their scale is by graphing the power values against the values 1 to 15, and then renaming the x-axis with the appropriate sample size. 

### Figure 1

```{r echo=FALSE}
knitr::include_graphics(c("GraphsFromPaper/fig1A.jpg","Figure1/fig1plotA.jpeg"))
```

```{r echo=FALSE}
knitr::include_graphics(c("GraphsFromPaper/fig1B.jpg","Figure1/fig1plotB.jpeg"))
```

```{r echo=FALSE}
knitr::include_graphics(c("GraphsFromPaper/fig1C.jpg","Figure1/fig1plotC.jpeg"))
```

### Figure 2

```{r echo=FALSE}
knitr::include_graphics(c("GraphsFromPaper/fig2A.jpg","Figure2/fig2plotA.jpeg"))
```

```{r echo=FALSE}
knitr::include_graphics(c("GraphsFromPaper/fig2B.jpg","Figure2/fig2plotB.jpeg"))
```

```{r echo=FALSE}
knitr::include_graphics(c("GraphsFromPaper/fig2C.jpg","Figure2/fig2plotC.jpeg"))
```

### Figure 3

```{r echo=FALSE}
knitr::include_graphics(c("GraphsFromPaper/fig3A.jpg","Figure3/fig3plotA.jpeg"))
```

```{r echo=FALSE}
knitr::include_graphics(c("GraphsFromPaper/fig3B.jpg","Figure3/fig3plotB.jpeg"))
```

## Error Analysis 

The methodology and design of the approval matrices was discussed above. Using this, an approval percentage (or rating) can be calculated, below they are in decimal form. The first table displays the approval of each distribution with and without the CSQ test. We deviated from the Yap's calculation of the CSQ test statistic, so it was expected that the CSQ powers did not match the paper's. This shows that across all distributions, our replication accuracy was much higher excluding the CSQ test. Furthermore, on multiple occasions we replicated the values in their paper to a high percentage for each test, noting that the asymptotic distributions (Weibull and Lognormal) had lower approval ratings. One last thing to note is that the Lognormal distribution had a low approval rating such that it was only approved for values near 1, meaning that no power values matched at low sample size. Whereas, the Weibull distribution had 100% approval for the Shapiro Wilks test and was hit and miss otherwise across the rest of the distributions. 

```{r echo=FALSE}
Distribution <- c("Uniform(0,1)", "GLD(0,1,1.25,1.25)","Trunc(-2,2)","t(15)","Logistic","Laplace","Weibull","Lognormal","LoConN(0.2,3)")
woCSQ <- c(0.9365, 0.9365, 0.9047, 0.9365, 0.8254, 0.9365, 0.4127, 0.3810, 0.7937)
wCSQ <- c(0.8611, 0.8875, 0.8056, 0.8333, 0.7222, 0.7639, 0.3750, 0.3611, 0.7222)

byDist <- data.frame(Distribution, woCSQ, wCSQ)
kable(byDist, "pipe")
```

Next, it was of interest to know which tests were getting power values near the paper's. As expected, the CSQ was the test with the least accuracy. The rest however, all hold above an 80% approval without the Lognormal distribution, with Shapiro-Wilks and Anderson-Darling leading the way around 95% approval. The percentages across the board tanked a little bit when including the Lognormal distribution, with the exception of the CSQ test. 

```{r echo=FALSE}
Test <- c("Shapiro Wilks", "Kolmogorov Smirnov", "Lilliefors", "Anderson Darling", "Jarque Bera", "D'Agostino Pearson", "Cramer von Mises", "Chi-Squared")
woLog <- c(0.9589, 0.7945, 0.8904, 0.9453, 0.9178, 0.7945, 0.8630, 0.1507)
wLog <- c(0.9024, 0.7319, 0.8292, 0.8902, 0.8659, 0.7561, 0.8048, 0.1585)

byTest <- data.frame(Test, woLog, wLog)
kable(byTest, "pipe")
```

The paper makes a claim about how the test acts on the types of distribution (short tailed, long tailed, and asymmetric). So, the approval ratings for each type of distribution for each test was calculated. This was done with the intention of seeing if this experiment's results match the claims made. If the approval ratings are high across the board, then it is easy to say that the claims match for both their work and ours. If the approval is low, then the claims may or may not match. Here we see that our work matches their claims for the long tail and short tail tests, with extremely high approval ratings all about 0.85. Thus, the power values we found were generally close to the actual papers. For the asymptotic distributions however, the approval rating is across the board with a high of 0.7778 and a low of 0.2963 excluding the CSQ test.


```{r echo=FALSE}
long_tail <- c(0.8519, 0.8889, 0.8148, 0.9630, 0.8889, 0.9260, 0.9630, 0.0370)
short_tail <- c(0.9286, 0.9286, 1.0000, 0.9643, 0.8571, 0.8929, 0.9286, 0.1786)
asymptotic <- c(0.7778, 0.2963, 0.5556, 0.5926, 0.4815, 0.5185, 0.4074, 0.1852)
byType <- data.frame(Test, long_tail, short_tail, asymptotic)
kable(byType, "pipe")
```

# Section 3: Summary

Overall, this project successfully replicate the power values for a large portion of the papers work that was shown. That said, the paper didn't provide many values to compare to in comparison to how many were calculated. It would have been nice to compare the values for the distributions in the graphs as well as for more sample sizes. The authors of the original work chose the values where the most interesting jumps happened, but it would be interesting to see our work compared on a grander scale to all the work the paper claimed; there were a few tests and distributions mentioned but not included in the paper as well. 

Our results do match the results of the paper for the short tail and long tailed distributions. This is seen by the approval ratings for the short tail and long tailed distributions being high. As for the asymptotic distributions, the approval rating was low but when looking at the graphs and tables for those distributions, it is clear that what the paper claimed still holds true. For the symmetric short tailed distributions,the best tests were Shapiro Wilks and D'Agostino pearson and for the symmetric long tailed distributions the best tests were Shapiro Wilks, D'Agostino Pearson, and Jarque-Bera. Finally, the most powerful distribution for the asymptotic distributions was the Shapiro Wilks test. This implies that the Shapiro Wilks test is the most robust test in the sense that it is the most consistent regardless of the distribution. 

# References

Research paper:
Yap, Bee Wah, and Chiaw Hock Sim. "Comparisons of various types of normality tests." Journal of Statistical Computation and Simulation 81.12 (2011): 2141-2155.

Book:
Rizzo, Maria L. Statistical computing with R. CRC Press, 2019.

Packages:

Diethelm Wuertz, Tobias Setz and Yohan Chalabi (2020). fBasics: Rmetrics - Markets and Basic Statistics. R package version 3042.89.1.
https://CRAN.R-project.org/package=fBasics

H. Wickham. ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York, 2016.

Hadley Wickham, Romain François, Lionel Henry and Kirill Müller (2021). dplyr: A Grammar of Data Manipulation. R package version 1.0.6.
https://CRAN.R-project.org/package=dplyr

Hao Zhu (2021). kableExtra: Construct Complex Table with 'kable' and Pipe Syntax. R package version 1.3.4. 
https://CRAN.R-project.org/package=kableExtra

Joseph L. Gastwirth, Yulia R. Gel, W. L. Wallace Hui, Vyacheslav Lyubchich, Weiwen Miao and Kimihiro Noguchi (2020). lawstat: Tools for Biostatistics, Public Policy, and Law. R package version 3.4.
https://CRAN.R-project.org/package=lawstat

Juergen Gross and Uwe Ligges (2015). nortest: Tests for Normality. R package version 1.0-4. https://CRAN.R-project.org/package=nortest

Olaf Mersmann, Heike Trautmann, Detlef Steuer and Björn Bornkamp (2018). truncnorm: Truncated Normal Distribution. R package version 1.0-8. 
https://CRAN.R-project.org/package=truncnorm

R Core Team (2021). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria.
URL https://www.R-project.org/.

Robert King, Benjamin Dean, Sigbert Klinke and Paul van Staden (2021). gld: Estimation and Use of the Generalised (Tukey) Lambda Distribution. R package version 2.6.3.
https://CRAN.R-project.org/package=gld

Thomas W. Yee (2015). Vector Generalized Linear and Additive Models: With an Implementation in R. New York, USA: Springer.

Thomas W. Yee and C. J. Wild (1996). Vector Generalized Additive Models. Journal of Royal Statistical Society, Series B, 58(3), 481-493.

Yihui Xie (2021). knitr: A General-Purpose Package for Dynamic Report Generation in R. R package version 1.33.

Yihui Xie (2015) Dynamic Documents with R and knitr. 2nd edition. Chapman and Hall/CRC. ISBN 978-1498716963

Yihui Xie (2014) knitr: A Comprehensive Tool for Reproducible Research in R. In Victoria Stodden, Friedrich Leisch and Roger D. Peng, editors, Implementing Reproducible Computational Research. Chapman and Hall/CRC. ISBN 978-1466561595
